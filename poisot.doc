<h1 id="introduction">Introduction</h1>
<p>The practice of science is becoming increasingly reliant on software --- despite the lack of formal training <span class="citation">(Hastings et al. <a href="#ref-hast14">2014</a>; Wilson et al. <a href="#ref-wils14">2014</a>)</span>, upwards of 30% of scientists need to <em>develop</em> their own. In ecology and evolution, this resulted in several journals (notably <em>Methods in Ecology &amp; Evolution</em>, <em>Ecography</em>, <em>BMC Ecology</em>) creating specific sections for papers describing software packages. This can only be viewed as a good thing, since the call to publish software in an open way has been made several times <span class="citation">(Barnes <a href="#ref-barn10">2010</a>)</span>, and is broadly viewed as a way towards greater reproducibility <span class="citation">(Ince et al. <a href="#ref-ince12">2012</a>)</span>. In addition, by providing a peer-reviewed, journal approved venue, this change in editorial practices gives credit to scientists for whom software development is a frequent research output.</p>
<p>Nevertheless, these papers are at best only pointers to the code, which is itself not tracked in a way that benefits software developers. While the long history of scientific publishing resulted in established (although debatable) measures for the impact of a scientist on the basis of its contribution to the literature, these is no similar effort or standard to do the same with software <span class="citation">(Howison and Bullard <a href="#ref-howi15">2015</a>)</span> -- as a result, while there exists an incentive to write good papers, there is no clear incentive to <em>write good software</em>. Some initiatives, such as <code>impactstory.org</code>'s metrics for code re-use, are a good start, but for code to be valued as a first-class research output, wider adoption of existing technologies and standards of quality is needed.</p>
<p>Most concerning is the fact that, despite the recognized need for quality control <span class="citation">(Baxter et al. <a href="#ref-baxt06">2006</a>)</span>, we do not currently have a set of community-wide best practices for how code should be released. While it is clear that code is on the verge of being perceived, valued, and tracked as a research output just like data and papers, understanding <em>how</em> it should be released and vetted, is still the great unknown. In a survey, <span class="citation">Joppa et al. (<a href="#ref-jopp13">2013</a>)</span> reported &quot;troubling trends&quot; in the use of software: only 30% of ecologists using species distribution modeling software justified their use by comparison with other tools, or previously published methods. My impression as a reviewer, and reader of the literature, is that software use tends to follow a trends, whereby if a package is picked up by a few authors, it will rapidly gain traction regardless of its actual <em>quality</em>.</p>
<p>One of the recommendations by <span class="citation">Joppa et al. (<a href="#ref-jopp13">2013</a>)</span> is that code should be reviewed. Yet as almost anyone that wrote or attempted to review scientific software will attest, this implies a tremendous effort. First, not all software is written in the same language. This simple fact makes looking for reviewers with the expertise to handle a paper incredibly difficult. In an already over-burdened peer-review system, restricting the search for referees to only people with the technical know-how to perform code review is sure to lengthen the review process. Second, even for the most trivial pieces of software, experienced code-reviewers are not able to follow all possible paths of program flow <span class="citation">(Uwano et al. <a href="#ref-uwan06">2006</a>)</span>; and it can easily be argued that very few ecologists are <em>experienced</em> code reviewers --- nor should they be.</p>
<p>In order to make scientific software better, which is to say, to both increase the confidence of users and minimize the chances of introducing hard to detect bugs --- which is a pre-requisite for making sure that writing code is valued in the same way that writing papers is, these two challenges must be addressed. The good thing is that solutions are <em>already</em> in place, and all that is needed is to increase their adoption by a broader share of the community.</p>
<h1 id="proposed-best-practices">Proposed best practices</h1>
<p>In the following sections, I will outline how a few steps can be used to make software more reliable, easier to re-use, and discoverable. These are not meant to be the end-all solution to software related issues, but rather to stimulate a discussion between software producers, software users, and journal editors. These steps are in addition to already well established best practices: the use of free and open-source license <span class="citation">Morin et al. (<a href="#ref-mori12">2012</a>)</span> (so that the raw source code can be used and improved upon by all users), and the use of publicly available repositories (so that the history of changes can easily be consulted).</p>
<h2 id="test-your-software">Test your software</h2>
<p>A test suite is a series of situations that test how the code responds to known inputs. For example, if one were to write a function to measure the mean of a series of number, a good test suite would ensure that the mean of <span class="math inline">3.0</span> and <span class="math inline">4.0</span> is <span class="math inline">3.5</span> (subtleties of floating-point arithmetic notwithstanding). An excellent test suite would ensure that the mean of <span class="math inline">3.0</span> and the character <code>&quot;a&quot;</code> is not defined, and send a message to the user explaining what went wrong, and how it can be fixed. Designing good test suites is somewhat of an art form, but <span class="citation">Hunt and Thomas (<a href="#ref-hunt99">1999</a>)</span> have a good description of how it can be done.</p>
<p>Writing explicit (and well documented) test suites and releasing them alongside the code has the potential to significantly reduce the reviewing effort. While reading through code requires the reviewer to be familiar (and even proficient) with a language, test packages usually involve a self-explanatory syntax. For example, <code>python</code>'s <code>assertEqual(mean(2, 3), 2.5)</code>, <code>julia</code>'s <code>@test mean(2, 3) 2.5</code>, and <code>R</code>'s <code>expect_equal(mean(2, 3), 2.5)</code> are all easy to understand, even if the details of how <code>mean</code> is implemented are not. Reading tests is also orders of magnitude faster than reading the code itself, and if the code is sufficiently covered, this should be enough to evaluate the robustness of the software.</p>
<h2 id="inform-users-of-the-test-coverage">Inform users of the test coverage</h2>
<p>Test engines, when running, collect informations about which lines were tested, how often, and which lines were not. This is important information, since it (roughly speaking) informs users of what proportion of the features they are about to use are known to perform as they should. <span class="citation">Yong Woo (<a href="#ref-yong03">2003</a>)</span> gives good evidence that code coverage analysis, over time, improves software quality.</p>
<p>From a publishing and reviewing point of view, coverage analysis is an incredibly powerful tool. While reviewing the entirety of a source code is difficult and not foolproof, it is much easier to look up what fraction of the code is covered (services like <code>coveralls.io</code> go as far as color-coding the page when the code is not properly covered), then to evaluate whether the test suite is exhaustive enough. Based on this information, reviewers can easily make recommendations about where code revision is needed.</p>
<h2 id="let-the-cloud-work-for-you">Let the cloud work for you</h2>
<p>While software developers will run test suite and coverage analyses on their machines, it is important to (i) report the results to the users and (ii) ensure that the software works on &quot;clean&quot; machines. This can be done, in a single step, using cloud-based services known as Continuous Integration (CI) engines. Continuous integration <span class="citation">(Duvall et al. <a href="#ref-duva07">2007</a>)</span> is the practice of committing every change to a source code to a service that will test whether or not the software still works.</p>
<p>This is usually done by coupling a continuous integration (CI) engine (such as, <em>e.g.</em>, <code>travis-ci.org</code>) to a version control system such as <code>git</code> or SVN <span class="citation">(Ram <a href="#ref-ram13">2013</a>)</span>. When a new change is sent to the central copy (hosted on the <code>git</code> server), the CI engine will run (roughly speaking) two steps. First, it will setup a new environment with the minimum amount of software and libraries needed to run the code. Then, it will run a user-specified series of steps (usually the test suite and coverage analysis), and if none on them fail, will report that the build (the latest version of the code) is &quot;passing&quot;. If not, the build will be &quot;failing&quot;.</p>
<p>Using CI engines serves two purposes. First, it proves that the software runs on other machines and configurations (most CI engines allow to run, <em>e.g.</em>, different versions of <code>R</code>), and (most importantly) that the dependencies are known and can be installed without effort. Second, it serves as a hub for other services. Most cloud-based solutions are well integrated to one another: sending a new version of the code to <em>GitHub</em> will trigger a build on <em>TravisCI</em>, which will perform the coverage analysis for <em>Coveralls</em> to report, and both will then send the results back to <em>GitHub</em> for the users to see. Not only does it publicly discloses two obvious measures of code quality, it does so in a way that is effortless for the developer.</p>
<h2 id="release-code-in-a-citable-way">Release code in a citable way</h2>
<p>As mentioned in the introduction, while software papers include links to the code, the code itself is not tracked, and is difficultly citable. This has the major disadvantage of not giving credit to software developers for their code (as opposed to for their papers describing the code). In addition, although papers are usually published once, software undergo many iterations (&quot;releases&quot;), and each of them should be cited as a separate entity. Citing code releases could be a leap forward for reproducibility. If a specific version of the code is used and cited, it becomes possible to reproduce the analysis in similar conditions (this assumes that the version number of dependencies is given too). Should a version, or range of versions, or a software be affected by a bug, this also provides a way to rapidly identify which papers can have been affected.</p>
<p><em>Zenodo</em> (<code>zenodo.org</code>) recently partnered with <em>GitHub</em>, to offer researchers the opportunity to get DOI (Digital Object Identifiers) for their code. Every time a new <em>release</em> of the code is created, it receives a new DOI, and can be cited as any other scientific document. This is a necessary step if we want to fully understand the impact of code on the scientific literature. <em>Zenodo</em> (hosted by the CERN Data Centre) offers independent and redundant copies of every published version, so one can decide which release to download and cite.</p>
<h2 id="write-documentation-publish-use-cases">Write documentation, publish use-cases</h2>
<p>Looking at recently published software papers in any journal, it is clear that there is no consensus on how these should be written; which is not necessarily a bad thing, but suggests that the community is trying to find its marks in this new practice. To some extent, software papers are a form of documentation. Yet, using them to document <em>how the program works</em>, as opposed to <em>what the program does</em>, feels like a missed opportunity. Most modern languages offer the possibility to extract formatted &quot;docstrings&quot; to compile a manual from the code itself. The <code>readthedocs.org</code> service does it automatically for <code>python</code>, and there are solution for <code>R</code> (<code>roxygenize</code>), <code>julia</code> (<code>Docile.jl</code>), and others languages. Since the &quot;technical manual&quot; can be extracted directly from the code, software papers are the place to showcase what the software can do. For example, the description of the <code>taxize</code> package for <code>R</code> <span class="citation">(Chamberlain and Szöcs <a href="#ref-cham13a">2013</a>)</span>, rather than giving a run down of the different functions, emphasizes how the package can be used for actual research questions, and links to the more extensive documentation.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The use of most of the tools mentioned (a summary of which is given in Table 1) can easily be made public. The <code>shields.io</code> service, for example, provides templates that can be copied/pasted into any web page, giving an up-to-date status information of CI builds, code coverage, link to the documentation, and DOI of the current release. With most journals moving to online-only, it would not be unreasonable to suggest that these, or similar, badges, be presented on the online version of the paper. This would give <em>readers</em> the insurance that the software they are going to read about has been tested, and is most likely to be robust than software about which nothing is known. While this can currently be done on the webpage of the project (see Figure 1), moving this information in the paper itself would send a strong signal that using these tools is actively encouraged.</p>
<p>If anything, the importance of code and software in day-to-day scientific practice will only increase, and this is a good thing (since it implies that most researchers are adopting state-of-the-art methodology). It is only important to remember that software is written by people, and people make mistakes. Taking simple precautions to make sure that the software works will undoubtedly accelerate the review process, and increase the overall quality of code. In parallel, adding unique identifiers on code, and focusing in describing what it does rather than how it does it, will make it easier to find, easier to cite, and easier to adopt <span class="citation">(Howison and Bullard <a href="#ref-howi15">2015</a>)</span>. In most fields in biology, reporting the controls is as important as reporting the outcome of the experiment -- there is no reason for software not to be held to the same standard, since the application of code is nothing if not an experiment.</p>
<p>With any standard comes the question of how, and by whom, it should be enforced. Ideally, any researcher writing code will recognize the benefits of these practices, and use them. In any case, there are two categories of people who can have a direct impact of their adoption. The first is, as often, reviewers and editors. Should informations about testing, code coverage, licenses, be presented in an easy to access way, it would be easy for reviewers to treat this information as another measure of the quality or suitability of a paper, or to recommend adjustments. Editors can implement journal-level policies, which spell out the requirements for a manuscript to be accepted. Finally, readers and software users can &quot;vote with their feet&quot;; if a piece of software does not gives enough information to trust it, the rational decision would be <em>not</em> to use it, and to reach out to the maintainers and request that the best practices are followed.</p>
<p>None of the above practices are an unreasonable time-sink, and they have the ability to make scientific software better and more reliable. Just as we want to have a high degree of confidence in the equipment we use in research, so too should we have high standards for the code we use to produce and analyze results. There are outstanding grass-root initiatives (<em>Mozilla Science Lab</em>, <code>http://mozillascience.org</code>; <em>Software Carpentry</em>, <code>http://www.software-carpentry.org/</code>) that are aimed at improving the computational literacy, and awareness of best practices, among researchers. Writing code is not a niche occupation for ecologists anymore, and existing training opportunities should be used to reflect this change in practices.</p>
<p><strong>Acknowledgments</strong> --- this paper was prepared when putting together notes for a workshop on code discoverability, for the Canadian Society of Ecology and Evolution annual meeting 2015, in Saskatoon, and largely inspired by group discussions in the Stouffer lab, University of Canterbury. TP is funded by a starting grant from the Université de Montréal, and a Discovery grant from NSERC. Thanks are due to Ethan White and Jeffrey Hollister for comments on the initial submission of this manuscript, as well as Robert Davey for suggestions.</p>
<p></p>
<p>Table 1: Summary of the different tools, along with URL and a short description of their purpose. All of them can be used at no cost for open source projects, and many also provide educational discount for which scientists are eligible.</p>
<table>
<thead>
<tr class="header">
<th align="left">Service</th>
<th align="left">URL</th>
<th align="left">Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><em>GitHub</em></td>
<td align="left"><code>github.com</code></td>
<td align="left">Version control</td>
</tr>
<tr class="even">
<td align="left"><em>GitLab</em></td>
<td align="left"><code>gitlab.com</code></td>
<td align="left">Version control</td>
</tr>
<tr class="odd">
<td align="left"><em>BitBucket</em></td>
<td align="left"><code>bitbucket.org</code></td>
<td align="left">Version control</td>
</tr>
<tr class="even">
<td align="left"><em>TravisCI</em></td>
<td align="left"><code>travis-ci.org</code></td>
<td align="left">Continuous integration (Linux)</td>
</tr>
<tr class="odd">
<td align="left"><em>Appveyor</em></td>
<td align="left"><code>appveyor.com</code></td>
<td align="left">Continuous integration (Windows)</td>
</tr>
<tr class="even">
<td align="left"><em>Jenkins</em></td>
<td align="left"><code>jenkins-ci.org</code></td>
<td align="left">Continuous integration (multi OS)</td>
</tr>
<tr class="odd">
<td align="left"><em>Coveralls</em></td>
<td align="left"><code>coveralls.io</code></td>
<td align="left">Code coverage analysis</td>
</tr>
<tr class="even">
<td align="left"><em>Codecov</em></td>
<td align="left"><code>codecov.io</code></td>
<td align="left">Code coverage analysis</td>
</tr>
<tr class="odd">
<td align="left"><em>Zenodo</em></td>
<td align="left"><code>zenodo.org</code></td>
<td align="left">DOI provider (<em>GitHub</em> integration)</td>
</tr>
<tr class="even">
<td align="left"><em>Shields</em></td>
<td align="left"><code>shields.io</code></td>
<td align="left">Badges to inform on code status</td>
</tr>
<tr class="odd">
<td align="left"><em>ImpactStory</em></td>
<td align="left"><code>impactstory.org</code></td>
<td align="left">Information on code impact</td>
</tr>
<tr class="even">
<td align="left"><em>ReadTheDocs</em></td>
<td align="left"><code>readthedocs.org</code></td>
<td align="left">Easy generation of documentation</td>
</tr>
</tbody>
</table>
<p></p>
<div class="figure">
<img src="ex.png" alt="Example of a GitHub hosted repository, with indication of the build status, current fraction of code covered by tests, and a link to the DOI provided by Zenodo. This informations helps users (and reviewers) evaluate how robustly the code has been tested, and whether it can easily be cited." />
<p class="caption">Example of a <em>GitHub</em> hosted repository, with indication of the build status, current fraction of code covered by tests, and a link to the DOI provided by <em>Zenodo</em>. This informations helps users (and reviewers) evaluate how robustly the code has been tested, and whether it can easily be cited.</p>
</div>
<p></p>
<div class="references">
<h1 id="references" class="unnumbered">References</h1>
<div id="ref-barn10">
<p>Barnes N. Publish your computer code: it is good enough. Nature. 2010 Oct;467(7317):753. </p>
</div>
<div id="ref-baxt06">
<p>Baxter SM, Day SW, Fetrow JS, Reisinger SJ. Scientific Software Development Is Not an Oxymoron. PLoS Comput Biol [Internet]. Public Library of Science (PLoS); 2006;2(9):e87. Available from: <a href="http://dx.doi.org/10.1371/journal.pcbi.0020087" class="uri">http://dx.doi.org/10.1371/journal.pcbi.0020087</a></p>
</div>
<div id="ref-cham13a">
<p>Chamberlain SA, Szöcs E. taxize: taxonomic search and retrieval in R. F1000Research. 2013 Oct; </p>
</div>
<div id="ref-duva07">
<p>Duvall PM, Matyas S, Glover A. Continuous integration: improving software quality and reducing risk. Pearson Education; 2007. </p>
</div>
<div id="ref-hast14">
<p>Hastings J, Haug K, Steinbeck C. Ten recommendations for software engineering in research. GigaScience [Internet]. 2014;3(1):31. Available from: <a href="http://www.gigasciencejournal.com/content/3/1/31" class="uri">http://www.gigasciencejournal.com/content/3/1/31</a></p>
</div>
<div id="ref-howi15">
<p>Howison J, Bullard J. Software in the scientific literature: Problems with seeing, finding, and using software mentioned in the biology literature. J Assn Inf Sci Tec [Internet]. Wiley-Blackwell; 2015 May;n/a￢ﾀﾓn/a. Available from: <a href="http://dx.doi.org/10.1002/asi.23538" class="uri">http://dx.doi.org/10.1002/asi.23538</a></p>
</div>
<div id="ref-hunt99">
<p>Hunt A, Thomas D. The Pragmatic Programmer: From Journeyman to Master. Addison-Wesley Professional; 1999. </p>
</div>
<div id="ref-ince12">
<p>Ince DC, Hatton L, Graham-Cumming J. The case for open computer programs. Nature. 2012 Feb;482(7386):485–8. </p>
</div>
<div id="ref-jopp13">
<p>Joppa LN, McInerny G, Harper R, Salido L, Takeda K, O’Hara K, et al. Troubling Trends in Scientific Software Use. Science. 2013 May;340(6134):814–5. </p>
</div>
<div id="ref-mori12">
<p>Morin A, Urban J, Sliz P. A Quick Guide to Software Licensing for the Scientist-Programmer. Lewitter F, editor. PLoS Comput Biol. 2012 Jul;8(7):e1002598. </p>
</div>
<div id="ref-ram13">
<p>Ram K. Git can facilitate greater reproducibility and increased transparency in science. Source Code Biol Med. 2013 Feb;8(1):7. </p>
</div>
<div id="ref-uwan06">
<p>Uwano H, Nakamura M, Monden A, Matsumoto K-i. Analyzing Individual Performance of Source Code Review Using Reviewers’ Eye Movement. Proceedings of 2006 symposium on Eye tracking research &amp; applications. 2006. </p>
</div>
<div id="ref-wils14">
<p>Wilson G, Aruliah DA, Brown CT, Chue Hong NP, Davis M, Guy RT, et al. Best Practices for Scientific Computing. PLoS Biol. 2014 Jan;12(1):e1001745. </p>
</div>
<div id="ref-yong03">
<p>Yong Woo K. Efficient Use of Code Coverage in Large-Scale Software Development. Proceedings of the 2003 conference of the Centre for Advanced Studies on Collaborative research. 2003. </p>
</div>
</div>
